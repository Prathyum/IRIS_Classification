{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn import svm \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('E:\\Kaggle Datasets\\Iris.csv') #reading your data *winks*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 6 columns):\n",
      "Id               150 non-null int64\n",
      "SepalLengthCm    150 non-null float64\n",
      "SepalWidthCm     150 non-null float64\n",
      "PetalLengthCm    150 non-null float64\n",
      "PetalWidthCm     150 non-null float64\n",
      "Species          150 non-null object\n",
      "dtypes: float64(4), int64(1), object(1)\n",
      "memory usage: 7.2+ KB\n"
     ]
    }
   ],
   "source": [
    " data.info()  #checking to see if I need to do any preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "My first ML project so documenting my steps\n",
    "\n",
    "1. Split the given data - Training data & Testing data\n",
    "2. Choose a suitable model (ML algo)\n",
    "3. Fit the Training data into the model to 'train' it [.fit()]\n",
    "4. Once trained, make the model predict the output using the Testing data [.predict()]\n",
    "5. Check accuracy by comparing predicted output and actual output\n",
    "\n",
    "Since this dataset is fairly simple and clean, there is no need for any pre-processing steps and hence\n",
    "there is no need to create a pipeline either. We can use straightforward ML to complete our task.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, test_size = 0.3)  #splitting my data to 70/30 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train[['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']] #the 'features' I want to train\n",
    "train_Y = train.Species #the o/p I want to predict (actual output)\n",
    "\n",
    "test_X = test[['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']] #make sure it has same features as training\n",
    "test_Y = test.Species #predicted output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM model accuracy =  0.9777777777777777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "model = svm.SVC()  #using SVM model\n",
    "\n",
    "model.fit(train_X, train_Y)  #train the model with training data\n",
    "pred = model.predict(test_X) #test the model with testing data by making predictions\n",
    "\n",
    "accuracy = metrics.accuracy_score(pred, test_Y)  #calc how accurate your model is\n",
    "print(\"SVM model accuracy = \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree model accuracy =  0.9111111111111111\n"
     ]
    }
   ],
   "source": [
    "model = DecisionTreeClassifier()\n",
    "\n",
    "model.fit(train_X, train_Y)\n",
    "pred = model.predict(test_X)\n",
    "\n",
    "accuracy = accuracy = metrics.accuracy_score(pred, test_Y)\n",
    "print(\"Decision Tree model accuracy = \" , accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "And that's about it. \n",
    "Since the dataset is quite small and has few features, the models ran fairly quick. Also the dataset had no missing values\n",
    "or categorical data and hence it was just a straightforward application of ML algorithms without the need for pre-processing.\n",
    "Obviously this is just the tip of the data-berg but I will soon be applying more complex algorithms on more tenacious datasets.\n",
    "\n",
    "GGWP\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
